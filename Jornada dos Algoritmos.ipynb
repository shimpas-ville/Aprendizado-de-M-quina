{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Conforme solicitado, iniciamos uma conversa com o ChatGPT a fim de conseguirmos desenvolver um algoritmo de treinamento com base em 3 diferentes métodos de aprendizagem, sendo eles: kNN, Naïve \nBayes e Árvores de Decisã.o\nO primeiro método escolhido foi o kNN, com o seguinte prompt:\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "Time:\n\n\"Gere um algoritmo para modelo de treinamento supervisionado kNN. Leve em consideração o seguinte: a linguagem utilizada deve ser Python; não deve ser feita a utilização de bibliotecas prontas além de numpy; o algoritmo deverá possuir comentários em sua estrutura que possibilitem a identifcação das etapas; seja o mais objetivo e simples na construção.\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = []\n        for x in X_test:\n            # Calcula a distância entre x e todos os pontos de treinamento\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n            # Ordena os índices das distâncias em ordem crescente\n            sorted_indices = np.argsort(distances)\n            # Seleciona os k vizinhos mais próximos\n            k_nearest_neighbors = sorted_indices[:self.k]\n            # Obtém as classes dos k vizinhos mais próximos\n            k_nearest_classes = self.y_train[k_nearest_neighbors]\n            # Determina a classe mais comum entre os vizinhos mais próximos\n            prediction = np.argmax(np.bincount(k_nearest_classes))\n            predictions.append(prediction)\n        return np.array(predictions)\n\n# Exemplo de uso:\nif __name__ == \"__main__\":\n    # Dados de treinamento\n    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y_train = np.array([0, 0, 1, 1])\n\n    # Dados de teste\n    X_test = np.array([[1, 1], [3, 3]])\n\n    # Instanciando e treinando o modelo\n    knn = KNN(k=2)\n    knn.fit(X_train, y_train)\n\n    # Fazendo previsões\n    predictions = knn.predict(X_test)\n    print(\"Predictions:\", predictions)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predictions: [0 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "raw",
      "source": "Após esta resposta, decidimos incrementar nosso modelo através do seguinte prompt:\n\nTime:\n\n\"Arrume este código a fim de que eu possa inserir os dados de treinamento facilmente e posteriormente inserir outro dado a fim de prever a qual classe ele pertence.\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = []\n        for x in X_test:\n            # Calcula a distância entre x e todos os pontos de treinamento\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n            # Ordena os índices das distâncias em ordem crescente\n            sorted_indices = np.argsort(distances)\n            # Seleciona os k vizinhos mais próximos\n            k_nearest_neighbors = sorted_indices[:self.k]\n            # Obtém as classes dos k vizinhos mais próximos\n            k_nearest_classes = self.y_train[k_nearest_neighbors]\n            # Determina a classe mais comum entre os vizinhos mais próximos\n            prediction = np.argmax(np.bincount(k_nearest_classes))\n            predictions.append(prediction)\n        return np.array(predictions)\n\nif __name__ == \"__main__\":\n    # Dados de treinamento\n    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y_train = np.array([0, 0, 1, 1])\n\n    # Instanciando e treinando o modelo\n    knn = KNN(k=2)\n    knn.fit(X_train, y_train)\n\n    # Dados de teste\n    X_test = np.array([[1, 1], [3, 3]])\n\n    # Fazendo previsões\n    predictions = knn.predict(X_test)\n    print(\"Predictions:\", predictions)\n\n    # Exemplo de previsão para um novo dado\n    new_data_point = np.array([[5, 5]])  # Novo dado para prever a classe\n    new_prediction = knn.predict(new_data_point)\n    print(\"New data point prediction:\", new_prediction)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": "Mesmo com a possibilidade de inserção de dados de treino, algo faltava para o nosso modelo. Portanto, solicitamos a criação de um gráfico que nos ajudasse a visualizar os resultados obtidos.\n\nTime:\n\n\"Crie um gráfico que exponha os dados de teste e o dado a ser previsto com cores diferentes.\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = []\n        for x in X_test:\n            # Calcula a distância entre x e todos os pontos de treinamento\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n            # Ordena os índices das distâncias em ordem crescente\n            sorted_indices = np.argsort(distances)\n            # Seleciona os k vizinhos mais próximos\n            k_nearest_neighbors = sorted_indices[:self.k]\n            # Obtém as classes dos k vizinhos mais próximos\n            k_nearest_classes = self.y_train[k_nearest_neighbors]\n            # Determina a classe mais comum entre os vizinhos mais próximos\n            prediction = np.argmax(np.bincount(k_nearest_classes))\n            predictions.append(prediction)\n        return np.array(predictions)\n\nif __name__ == \"__main__\":\n    # Dados de treinamento\n    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y_train = np.array([0, 0, 1, 1])\n\n    # Instanciando e treinando o modelo\n    knn = KNN(k=2)\n    knn.fit(X_train, y_train)\n\n    # Dados de teste\n    X_test = np.array([[1, 1], [3, 3]])\n\n    # Fazendo previsões\n    predictions = knn.predict(X_test)\n\n    # Plotando os dados de teste e o dado a ser previsto\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap='coolwarm', label='Dados de Teste')\n    plt.scatter(5, 5, c='black', marker='x', label='Novo Dado a Ser Previsto')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Dados de Teste e Dado a Ser Previsto')\n    plt.legend()\n    plt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": "Mesmo com o gráfico, o modelo permanecia carente de dados, então decidimos solictar a adição de mais amostras. \n\nTime: \n\n\"O modelo precisa ser treinado com no mínimo 30 dados diferentes, conserte isso.\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = []\n        for x in X_test:\n            # Calcula a distância entre x e todos os pontos de treinamento\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n            # Ordena os índices das distâncias em ordem crescente\n            sorted_indices = np.argsort(distances)\n            # Seleciona os k vizinhos mais próximos\n            k_nearest_neighbors = sorted_indices[:self.k]\n            # Obtém as classes dos k vizinhos mais próximos\n            k_nearest_classes = self.y_train[k_nearest_neighbors]\n            # Determina a classe mais comum entre os vizinhos mais próximos\n            prediction = np.argmax(np.bincount(k_nearest_classes))\n            predictions.append(prediction)\n        return np.array(predictions)\n\nif __name__ == \"__main__\":\n    # Dados de treinamento\n    X_train = np.random.rand(30, 2) * 10  # 30 amostras de treinamento\n    y_train = np.random.randint(2, size=30)  # Classes aleatórias (0 ou 1)\n\n    # Instanciando e treinando o modelo\n    knn = KNN(k=5)  # k = 5 para o exemplo\n    knn.fit(X_train, y_train)\n\n    # Dados de teste\n    X_test = np.random.rand(10, 2) * 10  # 10 amostras de teste\n\n    # Fazendo previsões\n    predictions = knn.predict(X_test)\n\n    # Plotando os dados de teste e o dado a ser previsto\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap='coolwarm', label='Dados de Teste')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Dados de Teste')\n    plt.colorbar(label='Classes Previstas')\n    plt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": "Com o gráfico sendo construído corretamente, partimos para uma análise mais minuciosa do nosso algorimto, e percebemos que a plotagem de classe estava sendo feita de forma aleatória.\n\nTime: \n\n\"O gráfico está plotando duas classe diferentes, entretanto não consigo entender qual o critério foi utilizado para diferenciar as duas classes. Explique.\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "\"Peço desculpas pelo mal-entendido. No código fornecido, as cores no gráfico representam as classes previstas pelo modelo de kNN. No entanto, como os rótulos das classes de treinamento são gerados aleatoriamente, as cores não têm uma correspondência específica com uma classe real. Em outras palavras, as cores são atribuídas às previsões feitas pelo modelo de kNN e não têm relação direta com as classes reais dos dados de teste.\n\nPara fazer uma análise mais significativa, seria necessário definir uma lógica para atribuir cores específicas a cada classe prevista. Como os rótulos das classes de treinamento são gerados aleatoriamente neste exemplo, a atribuição de cores não reflete uma separação real entre as classes.\n\nSe desejar, posso modificar o código para que as classes de treinamento tenham um padrão mais claro, o que tornaria a visualização mais significativa. Ou, se você tiver classes reais e gostaria de visualizar a distribuição das previsões do modelo com base nessas classes, podemos adaptar o código para isso também. Por favor, me avise como gostaria de proceder.\"",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "Sendo assim, decidimos criar dois grupos iniciais distintos, que corresponderiam a duas classe diferentes, e, posteriormente, inserir mais 50 dados aleatórios a fim de verificar se o modelo preveria as classe sdos dados novos corretamente com base nas diferenças dos dados iniciais.\n\nTime:\n\n\"Coloque dois grupos de 5 dados como amostra de treinamento. O primeiro grupo de dados deve somar sempre mais do que 10 na soma das coordenadas individuais, ou seja, todos devem ter soma x e y maior que 10. O segundo grupo deve ter todos os dados com soma x e y menor do que 10. Depois de treinar os modelos com estes dados, plote um gráfico. Depois de plotar este gáfico, insira mais 50 dados aleatórios para o modelo prever a quais classes pertencem, se a azul (menor que 10) ou à vermelha (maior que 10)\"\n\nObtivemos a seguinte resposta:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = []\n        for x in X_test:\n            # Calcula a distância entre x e todos os pontos de treinamento\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n            # Ordena os índices das distâncias em ordem crescente\n            sorted_indices = np.argsort(distances)\n            # Seleciona os k vizinhos mais próximos\n            k_nearest_neighbors = sorted_indices[:self.k]\n            # Obtém as classes dos k vizinhos mais próximos\n            k_nearest_classes = self.y_train[k_nearest_neighbors]\n            # Determina a classe mais comum entre os vizinhos mais próximos\n            prediction = np.argmax(np.bincount(k_nearest_classes))\n            predictions.append(prediction)\n        return np.array(predictions)\n\n    def add_new_data_point(self, new_data_point):\n        # Adiciona o novo dado ao conjunto de treinamento\n        self.X_train = np.vstack((self.X_train, new_data_point))\n        # Atualiza as classes dos dados de treinamento\n        new_data_class = np.sum(new_data_point) > 10\n        self.y_train = np.append(self.y_train, new_data_class)\n        # Re-treina o modelo com o novo dado\n        self.fit(self.X_train, self.y_train)\n\nif __name__ == \"__main__\":\n    # Criando os grupos de dados de treinamento\n    group1 = np.random.rand(5, 2) * 10  # Grupo com soma das coordenadas maior que 10\n    group2 = np.random.rand(5, 2) * 5   # Grupo com soma das coordenadas menor que 10\n\n    # Dados de treinamento e suas classes\n    X_train = np.vstack((group1, group2))\n    y_train = np.array([1]*5 + [0]*5)  # Grupo 1: classe 1 (maior que 10), Grupo 2: classe 0 (menor que 10)\n\n    # Instanciando e treinando o modelo\n    knn = KNN(k=3)  # k = 3 para o exemplo\n    knn.fit(X_train, y_train)\n\n    # Plotando os dados de treinamento\n    plt.scatter(group1[:, 0], group1[:, 1], c='red', label='Grupo 1 (Soma > 10)')\n    plt.scatter(group2[:, 0], group2[:, 1], c='blue', label='Grupo 2 (Soma <= 10)')\n\n    # Adicionando mais 50 dados aleatórios\n    X_test = np.random.rand(50, 2) * 10\n    predictions = knn.predict(X_test)\n\n    # Plotando os dados de teste e o dado a ser previsto\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap='coolwarm', marker='x', label='Novos Dados')\n\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Dados de Treinamento e Novos Dados de Teste')\n    plt.legend()\n    plt.colorbar(label='Classes Previstas')\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": "Por fim, solicitamos ao ChatGPT que ele nos fornecesse a distancia utilizada para a construção do algoritmo.\n\nTime:\n\n\"explique qual distância você utilizou para criar o algoritmo.\"",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "ChatGPT:\n\n\"No algoritmo k-Nearest Neighbors (kNN), a distância mais comum utilizada para calcular a proximidade entre os pontos é a distância euclidiana. \nEssencialmente, a distância euclidiana mede o comprimento do segmento de linha reta que une dois pontos no espaço. Quando usamos o kNN, calculamos a distância euclidiana entre o ponto que queremos classificar e todos os pontos de treinamento. Depois, selecionamos os k pontos mais próximos (ou seja, aqueles com as menores distâncias euclidianas) e usamos as classes desses k pontos para determinar a classe do ponto que estamos classificando.",
      "metadata": {}
    }
  ]
}